{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob2\n",
    "import requests, json\n",
    "from lxml import html\n",
    "from astropy.time import Time\n",
    "import pickle\n",
    "from astropy.table import Table\n",
    "from astropy.io import ascii\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "import astropy.units as u\n",
    "from astropy import constants as const\n",
    "import sys, getopt, argparse\n",
    "import re\n",
    "from time import sleep\n",
    "from astropy.io import fits\n",
    "from subprocess import call\n",
    "from lxml import html\n",
    "import webbrowser as wb\n",
    "from urllib.error import HTTPError\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "global TOKEN, BASEURL\n",
    "GETTOKEN = '8e31426a-f04d-4a32-b470-54102e2d568d'      # Fritz API Key\n",
    "BASEURL = 'https://fritz.science/'                     # Fritz base url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"54916f1700966b3bd325fc1189763d86512bda1d\"     # TNS API Key\n",
    "YOUR_BOT_ID=\"48869\"\n",
    "YOUR_BOT_NAME=\"ZTF_Bot1\"\n",
    "\n",
    "# TNS URLs for real uploads\n",
    "TNS_BASE_URL = \"https://www.wis-tns.org/api/\"\n",
    "upload_url = \"https://www.wis-tns.org/api/file-upload\"\n",
    "report_url = \"https://www.wis-tns.org/api/bulk-report\"\n",
    "reply_url = \"https://www.wis-tns.org/api/bulk-report-reply\"\n",
    "\n",
    "# SANDBOX URLs for TNS upload trials\n",
    "SAND_TNS_BASE_URL = \"https://sandbox.wis-tns.org/api/\"\n",
    "SAND_upload_url = \"https://sandbox.wis-tns.org/api/file-upload\"\n",
    "SAND_report_url = \"https://sandbox.wis-tns.org/api/bulk-report\"\n",
    "SAND_reply_url = \"https://sandbox.wis-tns.org/api/bulk-report-reply\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api(method, endpoint, data=None):\n",
    "    ''' Info : Basic API query, takes input the method (eg. GET, POST, etc.), the endpoint (i.e. API url) \n",
    "               and additional data for filtering\n",
    "        Returns : response in json format\n",
    "        CAUTION! : If the query doesn't go through, try putting the 'data' input in 'data' or 'params' \n",
    "                    argument in requests.request call\n",
    "    '''\n",
    "    headers = {'Authorization': f'token {GETTOKEN}'}\n",
    "    response = requests.request(method, endpoint, json=data, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "\n",
    "def get_source_api(ztfname):\n",
    "    ''' Info : Query a single source, takes input ZTF name\n",
    "        Returns : all basic data of that source (excludes photometry and spectra, \n",
    "                  includes redshift, classification, comments, etc.)\n",
    "    '''\n",
    "    url = BASEURL+'api/sources/'+ztfname+'?includeComments=true'\n",
    "    response = api('GET',url)\n",
    "    return response['data']    \n",
    "\n",
    "\n",
    "\n",
    "def get_group_ids(groupnames=['Redshift Completeness Factor', 'Census of the Local Universe Caltech']):\n",
    "    ''' Info : Query group ids of groups specified\n",
    "        Input : Name or names of groups in an array []\n",
    "        Returns : List of group  names and their group ids \n",
    "    '''\n",
    "    \n",
    "    url = BASEURL+'api/groups'\n",
    "    headers = {'Authorization': f'token {GETTOKEN}'}\n",
    "    groupnames = np.atleast_1d(groupnames)\n",
    "    grpids = []\n",
    "    for grpname in groupnames:\n",
    "        response = requests.request('GET',url,params={'name':grpname}, headers=headers).json()\n",
    "        answer = str(grpname)+' = '+str(response['data'][0]['id'])\n",
    "        grpids.append(answer)\n",
    "        \n",
    "    return grpids\n",
    "\n",
    "\n",
    "\n",
    "def get_number_of_sources(group_id, date):\n",
    "    ''' Info : Query number of sources saved in a group after a certain date\n",
    "        Input : group id, date [yyyy-mm-dd]\n",
    "        Returns : Number of sources saved after a given date to the specified group\n",
    "    '''\n",
    "    \n",
    "    url = BASEURL+'api/sources?saveSummary=true&group_ids='+group_id+'&savedAfter='+date+'T00:00:00.000001'\n",
    "    response = api('GET',url)\n",
    "    return len(response['data']['sources'])\n",
    "\n",
    "\n",
    "\n",
    "def get_group_sources(group_id, date):\n",
    "    ''' Info : Query all sources saved in a group after a certain date\n",
    "        Input : group id, date [yyyy-mm-dd]\n",
    "        Returns : List of jsons of all sources in group(s)\n",
    "        Comment : Takes a little time based on the date\n",
    "    '''\n",
    "    \n",
    "    sources = []\n",
    "    \n",
    "    for i in range (get_number_of_sources(group_id, date)):\n",
    "    \n",
    "        url = BASEURL+'api/sources?saveSummary=true&group_ids='+group_id+'&savedAfter='+date+'T00:00:00.000001'\n",
    "        response = api('GET',url)\n",
    "        ztfname = response['data']['sources'][i]['obj_id']\n",
    "        sources.append(ztfname)\n",
    "        \n",
    "    return sources \n",
    "\n",
    "\n",
    "\n",
    "def get_total_number_of_sources(group_id):\n",
    "    ''' Info : Query total number of sources saved in a group\n",
    "        Input : group id\n",
    "        Returns : Total number of sources saved in a group\n",
    "    '''\n",
    "    \n",
    "    url = BASEURL+'api/sources?saveSummary=true&group_ids='+group_id\n",
    "    response = api('GET',url)\n",
    "    return len(response['data']['sources'])\n",
    "\n",
    "\n",
    "def get_all_group_sources(group_id):\n",
    "    ''' Info : Query all sources saved in a group\n",
    "        Input : group id\n",
    "        Returns : List of jsons of all sources in group(s)\n",
    "        Comment : Takes a long time\n",
    "    '''\n",
    "    \n",
    "    sources = []\n",
    "    \n",
    "    for i in range (get_number_of_sources(group_id)):\n",
    "    \n",
    "        url = BASEURL+'api/sources?saveSummary=true&group_ids='+group_id\n",
    "        response = api('GET',url)\n",
    "        ztfname = response['data']['sources'][i]['obj_id']\n",
    "        sources.append(ztfname)\n",
    "        \n",
    "    return sources \n",
    "\n",
    "\n",
    "def get_IAUname(ztfname):\n",
    "    \n",
    "    ''' Info : Query the TNS name for any source\n",
    "        Input : ZTFname\n",
    "        Returns : ATname\n",
    "    '''\n",
    "    \n",
    "    url = BASEURL+'api/alerts_aux/'+ztfname\n",
    "    response = api('GET',url)\n",
    "    return response[\"data\"][\"cross_matches\"][\"TNS\"]\n",
    "\n",
    "\n",
    "def get_classification(ztfname):\n",
    "                        \n",
    "    ''' Info : Query the classification and classification date for any source\n",
    "        Input : ZTFname\n",
    "        Returns : Classification and Classification date\n",
    "        Comment : You need to choose the classification if there are multiple classifications\n",
    "    '''\n",
    "    \n",
    "    url = BASEURL+'api/sources/'+ztfname+'/classifications'\n",
    "    response = api('GET',url)\n",
    "    output = response['data']\n",
    "    \n",
    "    if (len(output)< 1):\n",
    "        classification = \"No Classification found\"\n",
    "        classification_date = \"None\"\n",
    "        probability = \"0\"\n",
    "    \n",
    "    if (len(output)==1):\n",
    "        \n",
    "        classification = response['data'][0]['classification']\n",
    "        probability = response['data'][0]['probability']\n",
    "        classification_date = response['data'][0]['created_at'].split('T')[0]\n",
    "        \n",
    "        if (probability <= 0.6):\n",
    "\n",
    "            print (\"\\nHeads up! Low probability classification! \\n\")\n",
    "            \n",
    "            classification = classification\n",
    "            probability =  probability\n",
    "            classification_date = classification_date\n",
    "        \n",
    "    if (len(output) > 1):\n",
    "        \n",
    "        classification = []\n",
    "        classification_date = []\n",
    "        probability = []\n",
    "                    \n",
    "        for i in range (len(output)):\n",
    "            \n",
    "            classify = response['data'][i]['classification']  \n",
    "            classify_date = response['data'][i]['created_at']\n",
    "            prob = response['data'][i]['probability']\n",
    "                    \n",
    "            classification.append(classify)\n",
    "            probability=np.append(probability, prob)\n",
    "            classification_date.append(classify_date)\n",
    "            \n",
    "                    \n",
    "        for i in range (len(classification)):\n",
    "        \n",
    "            print ((i+1),\")\", \"Classification: \", classification[i],  \"\\t Probability:\", str(probability[i]), \"\\t Classification date:\", classification_date[i].split('T')[0])\n",
    "\n",
    "        user_input = input(\"Choose classification: \")\n",
    "        \n",
    "        if (probability[int(user_input)-1] <= 0.6):\n",
    "\n",
    "            print (\"\\nHeads up! Low probability classification! \\n\")\n",
    "\n",
    "            classification = classification[int(user_input)-1]\n",
    "            probability = probability[int(user_input)-1]\n",
    "            classification_date = classification_date[int(user_input)-1].split('T')[0]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            classification = classification[int(user_input)-1]\n",
    "            probability = probability[int(user_input)-1]\n",
    "            classification_date = classification_date[int(user_input)-1].split('T')[0]        \n",
    "        \n",
    "    return classification, probability, classification_date\n",
    "\n",
    "\n",
    "def get_redshift(ztfname):\n",
    "                    \n",
    "    ''' Info : Query the redshift for any source\n",
    "        Input : ZTFname\n",
    "        Returns : redshift\n",
    "    '''\n",
    "    \n",
    "    url = BASEURL+'api/sources/'+ztfname\n",
    "    response = api('GET',url)\n",
    "    \n",
    "    redshift = response['data']['redshift']\n",
    "    \n",
    "    if (redshift == None):\n",
    "        redshift = \"No redshift found\"\n",
    "    \n",
    "    return redshift\n",
    "\n",
    "\n",
    "def get_TNS_information(ztfname):\n",
    "    \n",
    "    url = BASEURL+'api/sources/'+ztfname\n",
    "    response = api('GET',url)\n",
    "        \n",
    "    IAU = get_IAUname(ztfname)\n",
    "\n",
    "    if not IAU:\n",
    "        IAU = \"Not reported to TNS\"\n",
    "        \n",
    "    else:   \n",
    "        IAU = IAU[0]['name']\n",
    "              \n",
    "    clas = get_classification(ztfname)\n",
    "        \n",
    "    if clas[1] == 'None':\n",
    "        clas = \"Not classified yet\"\n",
    "        \n",
    "    else: \n",
    "        clas = ('Classification: '+str(clas[0])+','+ ' Probability: '+str(clas[1])+','+' Classification date: '+str(clas[2]))\n",
    "       \n",
    "           \n",
    "    redshift = get_redshift(ztfname)\n",
    "           \n",
    "    if redshift == None:\n",
    "        redshift = \"No redshift found\"\n",
    "        \n",
    "    else:\n",
    "        redshift = ('redshift:'+str(redshift))        \n",
    "        \n",
    "    return ztfname, IAU, clas, redshift\n",
    "\n",
    "   \n",
    "def convert_to_jd(date):\n",
    "\n",
    "    d = Time(date, format='fits')\n",
    "    dat = d.jd\n",
    "    return dat\n",
    "\n",
    "\n",
    "def get_spectrum_api(spectrum_id):\n",
    "    ''' Info : Query all spectra corresponding to a source, takes input ZTF name\n",
    "        Returns : list of spectrum jsons\n",
    "    '''\n",
    "    url = BASEURL+'api/spectrum/'+str(spectrum_id)\n",
    "    response = api('GET',url)\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_all_spectra_len(ztfname):\n",
    "    \n",
    "    url = BASEURL+'api/sources/'+ztfname+'/spectra'\n",
    "    response = api('GET',url)\n",
    "    return len(response['data']['spectra'])\n",
    "\n",
    "\n",
    "def get_all_spectra_id(ztfname):\n",
    "    ''' Info : Query all spectra corresponding to a source, takes input ZTF name\n",
    "        Returns : list of spectrum jsons\n",
    "    '''\n",
    "\n",
    "    spec_id = []\n",
    "    \n",
    "    for i in range (get_all_spectra_len(ztfname)):\n",
    "    \n",
    "        url = BASEURL+'api/sources/'+ztfname+'/spectra'\n",
    "        response = api('GET',url)\n",
    "        \n",
    "        specid = response['data']['spectra'][i]['id']\n",
    "        spec_id.append(specid)\n",
    "        \n",
    "    return spec_id\n",
    "\n",
    "\n",
    "def get_required_spectrum_id(ztfname):\n",
    "    \n",
    "    flag = 0\n",
    "    \n",
    "    spec = (get_all_spectra_len(ztfname))\n",
    "    \n",
    "    name = []\n",
    "    date = []\n",
    "    \n",
    "    if spec == 0:\n",
    "        \n",
    "        specid = \"No Spectra Found\"\n",
    "        flag = 1\n",
    "        \n",
    "    if flag == 0:\n",
    "        \n",
    "        spec_id = get_all_spectra_id(ztfname)\n",
    "        \n",
    "        for s in range (spec):\n",
    "            \n",
    "            url = BASEURL+'api/sources/'+ztfname+'/spectra'\n",
    "            response = api('GET',url)\n",
    "\n",
    "            spec_name = response['data']['spectra'][s]['original_file_filename']\n",
    "            spec_date = response['data']['spectra'][s]['observed_at']\n",
    "            \n",
    "            name.append(spec_name)\n",
    "            date.append(spec_date.split('T')[0])\n",
    "\n",
    "        print (\"Please choose from the following spectra: \\n\")\n",
    "\n",
    "        for i in range (len(name)):\n",
    "            print ((i+1),\")\", \"spectrum name: \", name[i], \"spectrum date:\", date[i])\n",
    "            \n",
    "        wb.open(BASEURL+'source/'+ztfname, new=2) \n",
    "\n",
    "        user_input = input(\"Choose spectrum to upload: \")\n",
    "\n",
    "        specid = spec_id[int(user_input)-1]\n",
    "\n",
    "    return specid\n",
    "\n",
    "\n",
    "\n",
    "def write_ascii_file(ztfname):\n",
    "    \n",
    "    specid = get_required_spectrum_id(ztfname)\n",
    "    \n",
    "    flag = 0\n",
    "    \n",
    "    if (specid == 'No Spectra Found'):\n",
    "        spectrum_name = 'No Spectra Found'\n",
    "        print (spectrum_name)\n",
    "        flag = 1\n",
    "    \n",
    "    if flag == 0:\n",
    "        \n",
    "        a = get_spectrum_api(specid)\n",
    "\n",
    "        inst = (a['data']['instrument_name'])\n",
    "        #print (inst)\n",
    "    \n",
    "        if inst == 'SEDM':\n",
    "        \n",
    "            header = (a['data']['altdata'])\n",
    "            path = os.getcwd()\n",
    "\n",
    "            s = (ztfname+'_'+str(header['OBSDATE'])+'_'+str(inst)+'.ascii')\n",
    "\n",
    "            with open(path+'/data/'+s,'w') as f:\n",
    "                f.write(a['data']['original_file_string'])\n",
    "            f.close()\n",
    "\n",
    "            #print (s,'\\n')\n",
    "            spectrum_name = s\n",
    "        \n",
    "    \n",
    "        if inst == 'SPRAT':\n",
    "        \n",
    "        \n",
    "            header = (a['data']['altdata'])\n",
    "            \n",
    "            path = os.getcwd()\n",
    "\n",
    "            s = (ztfname+'_'+str(header['OBSDATE'].split('T')[0])+'_'+str(inst)+'.ascii')\n",
    "\n",
    "            with open(path+'/data/'+s,'w') as f:\n",
    "                f.write(a['data']['original_file_string'])\n",
    "            f.close()\n",
    "\n",
    "            #print (s,'\\n')\n",
    "            spectrum_name = s\n",
    "            \n",
    "            \n",
    "        if inst == 'ALFOSC':\n",
    "        \n",
    "            OBSDATE = a['data']['observed_at'].split('T')[0]\n",
    "            path = os.getcwd()\n",
    "\n",
    "            s = (ztfname+'_'+str(OBSDATE)+'_'+str(inst)+'.ascii')\n",
    "\n",
    "            with open(path+'/data/'+s,'w') as f:\n",
    "                f.write(a['data']['original_file_string'])\n",
    "            f.close()\n",
    "\n",
    "            #print (s,'\\n')\n",
    "            spectrum_name = s\n",
    "\n",
    "\n",
    "        if inst == 'DBSP':\n",
    "            \n",
    "            wav = (a['data']['wavelengths'])\n",
    "            flux = (a['data']['fluxes'])\n",
    "            err = (a['data']['errors'])\n",
    "            \n",
    "            OBSDATE = a['data']['observed_at'].split('T')[0]\n",
    "\n",
    "            path = os.getcwd()\n",
    "\n",
    "            s = (ztfname+'_'+str(OBSDATE)+'_'+str(inst)+'.ascii')\n",
    "            \n",
    "            if err == None:\n",
    "                \n",
    "                with open(path+'/data/'+s,'w') as f:\n",
    "                \n",
    "                    for i in range(len(wav)):\n",
    "                        f.write(str(wav[i])+'\\t'+str(flux[i])+'\\n')\n",
    "                f.close()\n",
    "\n",
    "                #print (s,'\\n')\n",
    "                spectrum_name = s\n",
    "                \n",
    "            else:\n",
    "\n",
    "                with open(path+'/data/'+s,'w') as f:\n",
    "\n",
    "                    for i in range(len(wav)):\n",
    "                        f.write(str(wav[i])+'\\t'+str(flux[i])+'\\t'+str(err[i])+'\\n')\n",
    "                f.close()\n",
    "\n",
    "                #print (s,'\\n')\n",
    "                spectrum_name = s\n",
    "            \n",
    "            \n",
    "        if inst == 'DIS':\n",
    "\n",
    "            #obsdate = a['data']['original_file_string'].split('#')[6]\n",
    "            #a,b = obsdate.split(' ', 1)\n",
    "            #c,OBSDATE = b.split(' ', 1)\n",
    "            #OBSDATE = OBSDATE.split('T')[0]\n",
    "            \n",
    "            obsdate = a['data']['observed_at']\n",
    "            OBSDATE = obsdate.split('T')[0]\n",
    "            \n",
    "            path = os.getcwd()\n",
    "            path = path+'/data/'\n",
    "\n",
    "            s = (ztfname+'_'+str(OBSDATE)+'_'+str(inst)+'.ascii')\n",
    "            \n",
    "            a = get_spectrum_api(specid)\n",
    "\n",
    "            with open(path+s,'w') as f:\n",
    "                f.write(a['data']['original_file_string'])\n",
    "            f.close()\n",
    "\n",
    "            #print (s,'\\n')\n",
    "            spectrum_name = s\n",
    "            \n",
    "            \n",
    "        if inst == 'KAST':\n",
    "\n",
    "            \n",
    "            obsdate = a['data']['observed_at']\n",
    "            OBSDATE = obsdate.split('T')[0]\n",
    "            \n",
    "            path = os.getcwd()\n",
    "            path = path+'/data/'\n",
    "\n",
    "            s = (ztfname+'_'+str(OBSDATE)+'_'+str(inst)+'.ascii')\n",
    "            \n",
    "            a = get_spectrum_api(specid)\n",
    "\n",
    "            with open(path+s,'w') as f:\n",
    "                f.write(a['data']['original_file_string'])\n",
    "            f.close()\n",
    "\n",
    "            #print (s,'\\n')\n",
    "            spectrum_name = s\n",
    "            \n",
    "            \n",
    "        if inst == 'LRIS':\n",
    "            \n",
    "            wav = (a['data']['wavelengths'])\n",
    "            flux = (a['data']['fluxes'])\n",
    "            err = (a['data']['errors'])\n",
    "            \n",
    "            OBSDATE = a['data']['observed_at'].split('T')[0]\n",
    "\n",
    "            path = os.getcwd()\n",
    "\n",
    "            s = (ztfname+'_'+str(OBSDATE)+'_'+str(inst)+'.ascii')\n",
    "            \n",
    "            if err == None:\n",
    "                \n",
    "                with open(path+'/data/'+s,'w') as f:\n",
    "                \n",
    "                    for i in range(len(wav)):\n",
    "                        f.write(str(wav[i])+'\\t'+str(flux[i])+'\\n')\n",
    "                f.close()\n",
    "\n",
    "                #print (s,'\\n')\n",
    "                spectrum_name = s\n",
    "                \n",
    "            else:\n",
    "\n",
    "                with open(path+'/data/'+s,'w') as f:\n",
    "\n",
    "                    for i in range(len(wav)):\n",
    "                        f.write(str(wav[i])+'\\t'+str(flux[i])+'\\t'+str(err[i])+'\\n')\n",
    "                f.close()\n",
    "\n",
    "                #print (s,'\\n')\n",
    "                spectrum_name = s\n",
    "            \n",
    "    return spectrum_name, specid\n",
    "\n",
    "\n",
    "\n",
    "def APO(specid):\n",
    "    \n",
    "    a = get_spectrum_api(specid)\n",
    "    inst = (a['data']['instrument_name'])\n",
    "    \n",
    "    obsdate = a['data']['original_file_string']['observed_at']\n",
    "    \n",
    "    OBSDATE = obsdate\n",
    "    \n",
    "    a = get_spectrum_api(specid)\n",
    "\n",
    "    exptime = a['data']['original_file_string'].split('#')[9]\n",
    "    a,b = exptime.split(' ', 1)\n",
    "    c,EXPTIME = b.split(' ', 1)\n",
    "    \n",
    "    a = get_spectrum_api(specid)\n",
    "\n",
    "    observers = (a['data']['original_file_string'].split('#')[10])\n",
    "    a,b = observers.split(' ', 1)\n",
    "    c,OBSERVERS = b.split(' ', 1)\n",
    "    \n",
    "    a = get_spectrum_api(specid)\n",
    "\n",
    "    reducers = a['data']['original_file_string'].split('#')[11]\n",
    "    a,b = reducers.split(' ', 1)\n",
    "    c,d = b.split(' ', 1)\n",
    "    REDUCERS,e = d.split('\\n', 1)\n",
    "    \n",
    "    return OBSDATE.split(' \\n')[0], EXPTIME.split(' \\n')[0], OBSERVERS.split(' \\n')[0], REDUCERS\n",
    "\n",
    "\n",
    "def pprint(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    slightly more convenient function instead of print(get_pprint)\n",
    "\n",
    "    params:\n",
    "        *args (arguments to pass to get_pprint)\n",
    "        **kwargs (keyword arguments to pass to get_pprint)\n",
    "    \"\"\"\n",
    "    print(get_pprint(*args, **kwargs))\n",
    "    \n",
    "\n",
    "def post_comment(ztfname, text):\n",
    "    \n",
    "    \n",
    "    data = {  \"obj_id\": ztfname,\n",
    "              \"text\": text,\n",
    "           }\n",
    "\n",
    "    url = BASEURL+'api/comment'\n",
    "    \n",
    "    response = api('POST', url, data=data)\n",
    "    \n",
    "    return response\n",
    "\n",
    "def delete_spectrum(spectrum_id):\n",
    "    url = BASEURL+'api/spectrum/'+spectrum_id\n",
    "    \n",
    "    response = api('DELETE', url)\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def retrieve_spectrum(spectrum_id):\n",
    "    url = BASEURL+'api/spectrum/'+spectrum_id\n",
    "    \n",
    "    response = api('GET', url)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classi(ztfname):\n",
    "                        \n",
    "    ''' Info : Query the classification and classification date for any source\n",
    "        Input : ZTFname\n",
    "        Returns : Classification and Classification date\n",
    "        Comment : You need to choose the classification if there are multiple classifications\n",
    "    '''\n",
    "    try:\n",
    "        \n",
    "        url = BASEURL+'api/sources/'+ztfname+'/classifications'\n",
    "        response = api('GET',url)\n",
    "        output = response['data']\n",
    "\n",
    "        if (len(output)< 1):\n",
    "            classification = \"Not Classified\"\n",
    "            classification_date = \"None\"\n",
    "\n",
    "        if (len(output) >= 1):\n",
    "\n",
    "            classification = response['data'][(len(output)-1)]['classification']  \n",
    "            classification_date = response['data'][(len(output)-1)]['created_at']\n",
    "            \n",
    "    except KeyError as e:\n",
    "        classification = \"Error\"\n",
    "        classification_date = \"Error\"\n",
    "        \n",
    "    return classification, classification_date\n",
    "\n",
    "def get_TNSname(ztfname):\n",
    "    \n",
    "    ''' Info : Query the TNS name for any source\n",
    "        Input : ZTFname\n",
    "        Returns : ATname\n",
    "    '''\n",
    "    try:\n",
    "        \n",
    "        url = BASEURL+'api/alerts_aux/'+ztfname\n",
    "        response = api('GET',url)\n",
    "        IAU = response['data']['cross_matches']['TNS']\n",
    " \n",
    "        \n",
    "        if not IAU:\n",
    "            IAU = \"Not reported to TNS\"\n",
    "\n",
    "        else:   \n",
    "            IAU = IAU[0]['name']\n",
    "\n",
    "    except KeyError as e:\n",
    "        IAU = \"Error\"\n",
    "        \n",
    "    return IAU\n",
    "\n",
    "\n",
    "def get_number(group_id, date):\n",
    "    ''' Info : Query number of sources saved in a group after a certain date\n",
    "        Input : group id, date [yyyy-mm-dd]\n",
    "        Returns : Number of sources saved after a given date to the specified group\n",
    "    '''\n",
    "    \n",
    "    url = BASEURL+'api/sources?saveSummary=true&group_ids='+group_id+'&savedAfter='+date+'T00:00:00.000001'\n",
    "    response = api('GET',url)\n",
    "    return len(response['data']['sources'])\n",
    "\n",
    "\n",
    "\n",
    "def get_sources(group_id, date):\n",
    "    ''' Info : Query all sources saved in a group after a certain date\n",
    "        Input : group id, date [yyyy-mm-dd]\n",
    "        Returns : List of jsons of all sources in group(s)\n",
    "        Comment : Takes a little time based on the date\n",
    "    '''\n",
    "    \n",
    "    sources = []\n",
    "    \n",
    "    for i in range (get_number(group_id, date)):\n",
    "    \n",
    "        url = BASEURL+'api/sources?saveSummary=true&group_ids='+group_id+'&savedAfter='+date+'T00:00:00.000001'\n",
    "        response = api('GET',url)\n",
    "        ztfname = response['data']['sources'][i]['obj_id']\n",
    "        sources.append(ztfname)\n",
    "        \n",
    "    return sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sourceclassification(outfile):\n",
    "    \n",
    "    dat = '2021-07-01' #Specify the date from which you want to check the saved sources\n",
    "\n",
    "    path = 'https://fritz.science/api/sources?group_ids=41&saveSummary=true&savedAfter='+dat+'T00:00:00.000001'\n",
    " \n",
    "    response = api('GET',path)\n",
    "    \n",
    "    srcs = []\n",
    "    dates = []\n",
    "    classify = []\n",
    "    class_date = []\n",
    "    TNS = []\n",
    "    \n",
    "    listdir = os.getcwd()\n",
    "    f = open (listdir+'/'+outfile+'.ascii','w')\n",
    "    f.write('Source Name'+'\\t'+'TNS Name'+'\\t'+'Saved Date'+'\\t'+'Classification'+'\\t'+'Classification Date'+'\\n')\n",
    "    \n",
    "    for i in range (get_number('41', dat)):\n",
    "            \n",
    "        source_name = response['data']['sources'][i]['obj_id']\n",
    "        saved_date = response['data']['sources'][i]['saved_at']\n",
    "        classification = get_classi(source_name)[0]\n",
    "        date = get_classi(source_name)[1]\n",
    "        IAU = get_TNSname(source_name)\n",
    "            \n",
    "        #print (i, source_name)\n",
    "\n",
    "        srcs.append(source_name)\n",
    "        TNS.append(IAU)\n",
    "        dates.append(saved_date.split('T')[0])\n",
    "        classify.append(classification)\n",
    "        class_date.append(date.split('T')[0])\n",
    "\n",
    "        \n",
    "    output = sorted(zip(class_date, srcs, TNS, dates, classify), reverse=True)\n",
    "\n",
    "\n",
    "    for i in range (get_number('41', dat)):\n",
    "\n",
    "        f.write(output[i][1]+'\\t'+output[i][2]+'\\t'+output[i][3]+'\\t'+output[i][4]+'\\t'+output[i][0]+'\\n')\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a17b46fb4b26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msourceclassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'updated_classifications'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-b107b4c1115c>\u001b[0m in \u001b[0;36msourceclassification\u001b[0;34m(outfile)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0msource_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sources'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obj_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msaved_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sources'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'saved_at'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mclassification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_classi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_classi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mIAU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_TNSname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-3503993c62b7>\u001b[0m in \u001b[0;36mget_classi\u001b[0;34m(ztfname)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBASEURL\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'api/sources/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mztfname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/classifications'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-b936b34b56ef>\u001b[0m in \u001b[0;36mapi\u001b[0;34m(method, endpoint, data)\u001b[0m\n\u001b[1;32m      7\u001b[0m     '''\n\u001b[1;32m      8\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Authorization'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf'token {GETTOKEN}'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mca_cert_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mca_cert_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             ssl_context=context)\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_fingerprint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir)\u001b[0m\n\u001b[1;32m    345\u001b[0m             or IS_SECURETRANSPORT):\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mHAS_SNI\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mserver_hostname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         warnings.warn(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 \u001b[0mcnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWantReadError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/OpenSSL/SSL.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \"\"\"\n\u001b[0;32m-> 1914\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_do_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sourceclassification('updated_classifications')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TNS_classification_ID(classification):    \n",
    "\n",
    "    class_ids = {'Afterglow':23, 'AGN':29, 'CV':27, 'Galaxy':30, 'Gap':60, 'Gap I':61, 'Gap II':62, 'ILRT':25, 'Kilonova':70, 'LBV':24,'M dwarf':210, 'Nova':26, 'Novae':26, 'QSO':31, 'SLSN-I':18, 'Ic-SLSN':18, 'SLSN-II':19, 'SLSN-R':20, 'SN':1, 'I':2, 'Type I':2, 'I-faint':15, 'I-rapid':16, 'Ia':3, 'Ia-norm':3, 'Ia-91bg':103,'Ia-91T':104, 'Ia-CSM':106, 'Ia-pec':100, 'Ia-SC':102, 'Ia-02cx':105,\n",
    "                'Ib':4, 'Ib-norm':4, 'Ib-Ca-rich':8, 'Ib-pec':107, 'Ib/c':6, 'SN Ibn':9, 'Ibn':9, 'Ic':5, 'Ic-norm':5, 'Ic-BL':7, 'Ic-pec':108, 'II':10, 'Type II':10, 'II-norm':10,\n",
    "                'II-pec':110, 'IIb':14, 'IIL':12, 'IIn':13, 'IIn-pec':112, 'IIP':11, 'SN impostor':99, 'Std-spec':50, 'TDE':120,\n",
    "                'Varstar':28, 'WR':200, 'WR-WC':202, 'WR-WN':201, 'WR-WO':203, 'Other':0}\n",
    "    \n",
    "    #keys = np.array(class_ids.keys())\n",
    "    for keys in class_ids:\n",
    "        if (keys == classification):\n",
    "            classkey = class_ids[keys]\n",
    "            return classkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TNS_instrument_ID(classification):\n",
    "\n",
    "    inst_ids = {'DBSP':1, 'ALFOSC': 41, 'LRIS': 3, 'DIS': 70, 'SEDM': 149, 'SPRAT': 156, 'GMOS': 6, 'KAST': 10, 'LFC': 2, 'TSPEC': 109}\n",
    "\n",
    "    #keys = np.array(class_ids.keys())\n",
    "    for keys in inst_ids:\n",
    "        if (keys == inst):\n",
    "            instkey = inst_ids[keys]\n",
    "            return instkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNSClassificationReport:\n",
    "    def __init__(self):\n",
    "        self.name = ''\n",
    "        self.fitsName = ''\n",
    "        self.asciiName = ''\n",
    "        self.classifierName = ''\n",
    "        self.classificationID = ''\n",
    "        self.redshift = ''\n",
    "        self.classificationComments = ''\n",
    "        self.obsDate = ''\n",
    "        self.instrumentID = ''\n",
    "        self.expTime = ''\n",
    "        self.observers = ''\n",
    "        self.reducers = ''\n",
    "        self.specTypeID = ''\n",
    "        self.spectrumComments = ''\n",
    "        self.groupID = ''\n",
    "        self.spec_proprietary_period_value = ''\n",
    "        self.spec_proprietary_period_units = ''\n",
    "\n",
    "    def classificationDict(self):\n",
    "        classificationDict =  {\n",
    "            'classification_report' : {\n",
    "                '0' : {\n",
    "                    'name' : self.name,\n",
    "                    'classifier' : self.classifierName,\n",
    "                    'objtypeid' : self.classificationID,\n",
    "                    'redshift': self.redshift,\n",
    "                    'groupid' : self.groupID,\n",
    "                    'remarks' : self.classificationComments,\n",
    "                    'spectra' : {\n",
    "                        'spectra-group' : {\n",
    "                            '0' : {\n",
    "                                'obsdate' : self.obsDate,\n",
    "                                'instrumentid' : self.instrumentID,\n",
    "                                'exptime' : self.expTime,\n",
    "                                'observer' : self.observers,\n",
    "                                'reducer' : self.reducers,\n",
    "                                'spectypeid' : self.specTypeID,\n",
    "                                'ascii_file' : self.asciiName,\n",
    "                                'fits_file' : self.fitsName,\n",
    "                                'remarks' : self.spectrumComments,\n",
    "                                'spec_proprietary_period' : {\n",
    "                                    'spec_proprietary_period_value' : self.spec_proprietary_period_value,\n",
    "                                    'spec_proprietary_period_units' : self.spec_proprietary_period_units\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return classificationDict\n",
    "\n",
    "    def classificationJson(self):\n",
    "        return json.dumps(self.classificationDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_TNS(filename, base_url = upload_url, api_key = API_KEY, filetype='ascii'):\n",
    "    \n",
    "    \"\"\"\n",
    "    uploads a file to TNS and returns the response json\n",
    "    \"\"\"\n",
    "    url = base_url\n",
    "    data = {'api_key' : api_key}\n",
    "    \n",
    "    headers={'User-Agent':'tns_marker{\"tns_id\":'+str(YOUR_BOT_ID)+', \"type\":\"bot\", \"name\":\"'+YOUR_BOT_NAME+'\"}'}\n",
    "\n",
    "    if filetype is 'ascii':\n",
    "        files = [('files[]', (filename, open(filename), 'text/plain'))]\n",
    "\n",
    "    elif filetype is 'fits':\n",
    "        files = [('files[0]', (filename, open(filename, 'rb'),\n",
    "                               'application/fits'))]\n",
    "\n",
    "    if filename:\n",
    "        response = requests.post(url, headers=headers, data=data, files=files)\n",
    "        try:\n",
    "            return response.json()\n",
    "        except:\n",
    "            print(url, data, files, response.content, sep='\\n')\n",
    "            return False\n",
    "    else:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tns_classify(classificationReport, base_url= report_url, api_key=API_KEY):\n",
    "    \"\"\"\n",
    "    submits classification report to TNS and returns the response json\n",
    "    \"\"\"\n",
    "    url = base_url\n",
    "    \n",
    "    headers={'User-Agent':'tns_marker{\"tns_id\":'+str(YOUR_BOT_ID)+', \"type\":\"bot\", \"name\":\"'+YOUR_BOT_NAME+'\"}'}  \n",
    "    \n",
    "    data = {'api_key' : api_key, 'data' : classificationReport.classificationJson()}\n",
    "    response = requests.post(url, headers=headers, data=data).json()\n",
    "    if not response:\n",
    "        return False\n",
    "\n",
    "    res_code = response['id_code']\n",
    "    report_id = response['data']['report_id']\n",
    "    print(\"ID:\", report_id)\n",
    "    print(res_code, response['id_message'], \"reporting finished\")\n",
    "    if res_code == 200:\n",
    "        return report_id\n",
    "    else:\n",
    "        print(\"Result reporting didn't work\")\n",
    "        pprint(response)\n",
    "        print(\"re-submit classification, but don't re-upload files\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tns_feedback(report_id):\n",
    "    data = {'api_key': API_KEY, 'report_id': report_id}\n",
    "    \n",
    "    headers={'User-Agent':'tns_marker{\"tns_id\":'+str(YOUR_BOT_ID)+', \"type\":\"bot\", \"name\":\"'+YOUR_BOT_NAME+'\"}'}\n",
    "    \n",
    "    response = requests.post(reply_url, headers=headers, data=data).json()\n",
    "    \n",
    "    feedback_code = response['id_code']\n",
    "    print(feedback_code, response['id_message'], \"feedback finished\")\n",
    "    if feedback_code == 200:\n",
    "        return True\n",
    "    elif feedback_code == 404:\n",
    "        print(\"Waiting and retrying...\")\n",
    "        sleep(2)\n",
    "        try:\n",
    "            return tns_feedback(report_id)\n",
    "        except KeyboardInterrupt:\n",
    "            return False\n",
    "    elif feedback_code == 400:\n",
    "        print(response)\n",
    "        return False\n",
    "    else:\n",
    "        # error receiving the feedback from TNS about the upload\n",
    "        print(\"Something went wrong with the feedback, but the report may\",\n",
    "              \"still have been fine?\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Heads up! Low probability classification! \n",
      "\n",
      "('ZTF21aazgkjf', 'SN 2021lhy', 'Classification: IIn, Probability: 0.5, Classification date: 2021-05-22', 'redshift:0.142') \n",
      "\n",
      "Do you want to proceed with the report?\n",
      "y/n: n\n",
      "('ZTF21aavgnld', 'AT 2021nhh', 'Classification: IIn, Probability: 1.0, Classification date: 2021-06-30', 'redshift:0.135') \n",
      "\n",
      "Do you want to proceed with the report?\n",
      "y/n: n\n",
      "('ZTF21ablxfwc', 'AT 2021thu', 'Classification: Ia, Probability: 1.0, Classification date: 2021-07-16', 'redshift:0.0212680008') \n",
      "\n",
      "Do you want to proceed with the report?\n",
      "y/n: n\n"
     ]
    }
   ],
   "source": [
    "#f = ascii.read(\"Sources.ascii\") #ascii file containing the names of sources and their saved dates\n",
    "#sources = f['col1']\n",
    "\n",
    "sources = ['ZTF21abjamta', 'ZTF21abjzkjf', 'ZTF21aazgkjf', 'ZTF21aavgnld', 'ZTF21ablxfwc'] \n",
    "           \n",
    "           \n",
    "for source in sources:\n",
    "    \n",
    "    flag = 0\n",
    "    ztfname = source\n",
    "    \n",
    "    for i in range (len(get_source_api(source)['comments'])):\n",
    "        \n",
    "        comment = get_source_api(source)['comments'][i]['text']\n",
    "        \n",
    "        if comment == 'Uploaded to TNS':\n",
    "            flag = 1\n",
    "        \n",
    "    if flag == 0:\n",
    "        \n",
    "        info = get_TNS_information(ztfname)\n",
    "        \n",
    "        k,l = (info[2]).split(':', 1)               #This is just to get the classification\n",
    "        m,n = l.split(',', 1)\n",
    "        o,p = m.split(' ', 1)\n",
    "        p,classify = m.split(' ', 1)\n",
    "        \n",
    "        if classify == 'Not classified yet':         #Check if classified\n",
    "            flag =1\n",
    "            continue  \n",
    "        \n",
    "        \n",
    "        a,b = (info[2]).split('Classification date', 1)             #This is just to get the classification date\n",
    "        c,d = b.split(':', 1)\n",
    "        e,class_date = d.split(' ', 1)\n",
    " \n",
    "\n",
    "        k,l = (info[2]).split(':', 1)               #This is just to get the classification\n",
    "        m,n = l.split(',', 1)\n",
    "        o,p = m.split(' ', 1)\n",
    "        p,classify = m.split(' ', 1)\n",
    "\n",
    "        #print (classify, class_date)\n",
    "\n",
    "\n",
    "        print (info, '\\n')\n",
    "        print (\"Do you want to proceed with the report?\")\n",
    "\n",
    "        user_input = input(\"y/n: \")\n",
    "\n",
    "        if user_input == 'y':\n",
    "\n",
    "            spectrum_info = write_ascii_file(ztfname) #returns \"spectrum_name\"\n",
    "\n",
    "            spectrum_name = spectrum_info[0]\n",
    "\n",
    "            if spectrum_name == 'No Spectra Found':\n",
    "                flag = 1\n",
    "                continue\n",
    "\n",
    "            if flag == 0:\n",
    "\n",
    "                path = os.getcwd()\n",
    "\n",
    "                specfile = (path+'/data/'+spectrum_name)\n",
    "\n",
    "                files = specfile\n",
    "\n",
    "                specid = spectrum_info[1]\n",
    "\n",
    "                a = get_spectrum_api(specid)\n",
    "\n",
    "                inst = (a['data']['instrument_name'])\n",
    "\n",
    "\n",
    "                if inst == 'SEDM':\n",
    "\n",
    "                    classifiers = 'A. Dahiwale, C. Fremling(Caltech) on behalf of the Zwicky Transient Facility (ZTF)'### Change accordingly\n",
    "                    source_group = 48 ### Require source group id from drop down list, 0 is for None\n",
    "                    spectypes = np.array(['object','host','sky','arcs','synthetic'])\n",
    "\n",
    "                    #proprietary_period = int(input(\"Proprietary period in years:\", x)\n",
    "                    proprietary_period = '0'\n",
    "                    proprietary_units = \"years\"\n",
    "                    spec_comments =''\n",
    "                    classification_comments = ''\n",
    "                    spectype='object'\n",
    "                    spectype_id = ['object', 'host', 'sky', 'arcs', 'synthetic'].index(spectype) + 1\n",
    "\n",
    "                    header = (a['data']['altdata'])\n",
    "                    obsdate = str((header['UTC']).split('T')[0])+' '+str((header['UTC']).split('T')[1])\n",
    "\n",
    "                    classificationReport = TNSClassificationReport()\n",
    "                    classificationReport.name = get_IAUname(ztfname)[0]['name'][3:]  \n",
    "                    classificationReport.fitsName = ''\n",
    "                    classificationReport.asciiName = spectrum_name\n",
    "                    classificationReport.classifierName = classifiers\n",
    "                    classificationReport.classificationID = get_TNS_classification_ID(classify)\n",
    "                    classificationReport.redshift = get_redshift(ztfname)\n",
    "                    classificationReport.classificationComments = classification_comments\n",
    "                    classificationReport.obsDate = obsdate\n",
    "                    classificationReport.instrumentID = get_TNS_instrument_ID(inst)\n",
    "                    classificationReport.expTime = (header['EXPTIME'])\n",
    "                    classificationReport.observers = 'SEDmRobot'\n",
    "                    classificationReport.reducers = (header['REDUCER'])\n",
    "                    classificationReport.specTypeID = spectype_id\n",
    "                    classificationReport.spectrumComments = spec_comments\n",
    "                    classificationReport.groupID = source_group\n",
    "                    classificationReport.spec_proprietary_period_value = proprietary_period\n",
    "                    classificationReport.spec_proprietary_period_units = proprietary_units\n",
    "                    print(classificationReport.classificationDict())\n",
    "\n",
    "                    #pprint(classificationReport.fill(), tab='  ')\n",
    "                    proceed = input(\"\\nProceed with classification and upload? ([y]/n) : \")\n",
    "                    if proceed == 'y' and not proceed.strip() == '':\n",
    "\n",
    "                        # ASCII FILE UPLOAD\n",
    "                        print (\"\\n\")\n",
    "                        response = upload_to_TNS(files)\n",
    "                        print (response)\n",
    "\n",
    "                        if not response:\n",
    "                            print(\"File upload didn't work\")\n",
    "                            print(response)\n",
    "                            #return False\n",
    "\n",
    "                        print(response['id_code'], response['id_message'],\n",
    "                              \"\\nSuccessfully uploaded ascii spectrum\")\n",
    "                        #classificationReport.asciiName = response['data'][-1]\n",
    "\n",
    "                        report_id = tns_classify(classificationReport)\n",
    "                        tns_feedback(report_id)\n",
    "                        post_comment(ztfname, 'Uploaded to TNS')\n",
    "\n",
    "\n",
    "                if inst == 'SPRAT':\n",
    "\n",
    "                    classifiers = 'D. A. Perley, K. Taggart (LJMU), A. Dahiwale, C. Fremling (Caltech) on behalf of the Zwicky Transient Facility (ZTF)'### Change accordingly\n",
    "                    source_group = 48 ### Require source group id from drop down list, 0 is for None\n",
    "                    spectypes = np.array(['object','host','sky','arcs','synthetic'])\n",
    "\n",
    "                    #proprietary_period = int(input(\"Proprietary period in years:\", x)\n",
    "                    proprietary_period = '0'\n",
    "                    proprietary_units = \"years\"\n",
    "                    spec_comments =''\n",
    "                    classification_comments = ''\n",
    "                    spectype='object'\n",
    "                    spectype_id = ['object', 'host', 'sky', 'arcs', 'synthetic'].index(spectype) + 1\n",
    "\n",
    "                    header = (a['data']['altdata'])\n",
    "                    obsdate = str(header['OBSDATE'].split('T')[0])+' '+str(header['OBSDATE'].split('T')[1])\n",
    "\n",
    "                    classificationReport = TNSClassificationReport()\n",
    "                    classificationReport.name = get_IAUname(ztfname)[0]['name'][3:] \n",
    "                    classificationReport.fitsName = ''\n",
    "                    classificationReport.asciiName = spectrum_name\n",
    "                    classificationReport.classifierName = classifiers\n",
    "                    classificationReport.classificationID = get_TNS_classification_ID(classify)\n",
    "                    classificationReport.redshift = get_redshift(ztfname)\n",
    "                    classificationReport.classificationComments = classification_comments\n",
    "                    classificationReport.obsDate = obsdate\n",
    "                    classificationReport.instrumentID = get_TNS_instrument_ID(inst)\n",
    "                    classificationReport.expTime = (header['EXPTIME'])\n",
    "                    classificationReport.observers = 'LTRobot'\n",
    "                    classificationReport.reducers = 'D. Perley'\n",
    "                    classificationReport.specTypeID = spectype_id\n",
    "                    classificationReport.spectrumComments = spec_comments\n",
    "                    classificationReport.groupID = source_group\n",
    "                    classificationReport.spec_proprietary_period_value = proprietary_period\n",
    "                    classificationReport.spec_proprietary_period_units = proprietary_units\n",
    "                    print(classificationReport.classificationDict())\n",
    "\n",
    "                    #pprint(classificationReport.fill(), tab='  ')\n",
    "                    proceed = input(\"\\nProceed with classification and upload? ([y]/n) : \")\n",
    "                    if proceed == 'y' and not proceed.strip() == '':\n",
    "\n",
    "                        # ASCII FILE UPLOAD\n",
    "                        print (\"\\n\")\n",
    "                        response = upload_to_TNS(files)\n",
    "                        print (response)\n",
    "\n",
    "                        if not response:\n",
    "                            print(\"File upload didn't work\")\n",
    "                            print(response)\n",
    "                            #return False\n",
    "\n",
    "                        print(response['id_code'], response['id_message'],\n",
    "                              \"\\nSuccessfully uploaded ascii spectrum\")\n",
    "                        #classificationReport.asciiName = response['data'][-1]\n",
    "\n",
    "                        report_id = tns_classify(classificationReport)\n",
    "                        tns_feedback(report_id)\n",
    "                        post_comment(ztfname, 'Uploaded to TNS')\n",
    "\n",
    "\n",
    "\n",
    "                if inst == 'ALFOSC':\n",
    "\n",
    "                    classifiers = 'A. Dahiwale, C. Fremling(Caltech) on behalf of the Zwicky Transient Facility (ZTF)'### Change accordingly\n",
    "                    source_group = 48 ### Require source group id from drop down list, 0 is for None\n",
    "                    spectypes = np.array(['object','host','sky','arcs','synthetic'])\n",
    "\n",
    "                    #proprietary_period = int(input(\"Proprietary period in years:\", x)\n",
    "                    proprietary_period = '0'\n",
    "                    proprietary_units = \"years\"\n",
    "                    spec_comments =''\n",
    "                    classification_comments = ''\n",
    "                    spectype='object'\n",
    "                    spectype_id = ['object', 'host', 'sky', 'arcs', 'synthetic'].index(spectype) + 1\n",
    "\n",
    "                    header = (a['data']['altdata'])\n",
    "                    obsdate = str(a['data']['observed_at'].split('T')[0])+' '+str(a['data']['observed_at'].split('T')[1])\n",
    "\n",
    "                    classificationReport = TNSClassificationReport()\n",
    "                    classificationReport.name = get_IAUname(ztfname)[0]['name'][3:]  \n",
    "                    classificationReport.fitsName = ''\n",
    "                    classificationReport.asciiName = spectrum_name\n",
    "                    classificationReport.classifierName = classifiers\n",
    "                    classificationReport.classificationID = get_TNS_classification_ID(classify)\n",
    "                    classificationReport.redshift = get_redshift(ztfname)\n",
    "                    classificationReport.classificationComments = classification_comments\n",
    "                    classificationReport.obsDate = obsdate\n",
    "                    classificationReport.instrumentID = get_TNS_instrument_ID(inst)\n",
    "                    classificationReport.expTime = (header['EXPTIME'])\n",
    "                    classificationReport.observers = (str(a['data']['observers'][0]['first_name'])+' '+str(a['data']['observers'][0]['last_name']))\n",
    "                    classificationReport.reducers = (str(a['data']['reducers'][0]['first_name'])+' '+str(a['data']['reducers'][0]['last_name']))\n",
    "                    classificationReport.specTypeID = spectype_id\n",
    "                    classificationReport.spectrumComments = spec_comments\n",
    "                    classificationReport.groupID = source_group\n",
    "                    classificationReport.spec_proprietary_period_value = proprietary_period\n",
    "                    classificationReport.spec_proprietary_period_units = proprietary_units\n",
    "                    print(classificationReport.classificationDict())\n",
    "\n",
    "                    #pprint(classificationReport.fill(), tab='  ')\n",
    "                    proceed = input(\"\\nProceed with classification and upload? ([y]/n) : \")\n",
    "                    if proceed == 'y' and not proceed.strip() == '':\n",
    "\n",
    "                        # ASCII FILE UPLOAD\n",
    "                        print (\"\\n\")\n",
    "                        response = upload_to_TNS(files)\n",
    "                        print (response)\n",
    "\n",
    "                        if not response:\n",
    "                            print(\"File upload didn't work\")\n",
    "                            print(response)\n",
    "                            #return False\n",
    "\n",
    "                        print(response['id_code'], response['id_message'],\n",
    "                              \"\\nSuccessfully uploaded ascii spectrum\")\n",
    "                        #classificationReport.asciiName = response['data'][-1]\n",
    "\n",
    "                        report_id = tns_classify(classificationReport)\n",
    "                        tns_feedback(report_id)\n",
    "                        post_comment(ztfname, 'Uploaded to TNS')\n",
    "\n",
    "\n",
    "\n",
    "                if inst == 'DBSP':\n",
    "\n",
    "                    classifiers = 'A. Dahiwale, C. Fremling(Caltech) on behalf of the Zwicky Transient Facility (ZTF)'### Change accordingly\n",
    "                    source_group = 48 ### Require source group id from drop down list, 0 is for None\n",
    "                    spectypes = np.array(['object','host','sky','arcs','synthetic'])\n",
    "\n",
    "                    #proprietary_period = int(input(\"Proprietary period in years:\", x)\n",
    "                    proprietary_period = '0'\n",
    "                    proprietary_units = \"years\"\n",
    "                    spec_comments =''\n",
    "                    classification_comments = ''\n",
    "                    spectype='object'\n",
    "                    spectype_id = ['object', 'host', 'sky', 'arcs', 'synthetic'].index(spectype) + 1\n",
    "\n",
    "                    OBSDATE = str(a['data']['observed_at'].split('T')[0])+' '+str(a['data']['observed_at'].split('T')[1])\n",
    "\n",
    "                    classificationReport = TNSClassificationReport()\n",
    "                    classificationReport.name = get_IAUname(ztfname)[0]['name'][3:]  \n",
    "                    classificationReport.fitsName = ''\n",
    "                    classificationReport.asciiName = spectrum_name\n",
    "                    classificationReport.classifierName = classifiers\n",
    "                    classificationReport.classificationID = get_TNS_classification_ID(classify)\n",
    "                    classificationReport.redshift = get_redshift(ztfname)\n",
    "                    classificationReport.classificationComments = classification_comments\n",
    "                    classificationReport.obsDate = OBSDATE\n",
    "                    classificationReport.instrumentID = get_TNS_instrument_ID(inst)\n",
    "                    #classificationReport.expTime = '450'\n",
    "                    classificationReport.observers = (str(a['data']['observers'][0]['first_name'])+' '+str(a['data']['observers'][0]['last_name']))\n",
    "                    classificationReport.reducers = (str(a['data']['reducers'][0]['first_name'])+' '+str(a['data']['reducers'][0]['last_name']))\n",
    "                    classificationReport.specTypeID = spectype_id\n",
    "                    classificationReport.spectrumComments = spec_comments\n",
    "                    classificationReport.groupID = source_group\n",
    "                    classificationReport.spec_proprietary_period_value = proprietary_period\n",
    "                    classificationReport.spec_proprietary_period_units = proprietary_units\n",
    "                    print(classificationReport.classificationDict())\n",
    "\n",
    "                    #pprint(classificationReport.fill(), tab='  ')\n",
    "                    proceed = input(\"\\nProceed with classification and upload? ([y]/n) : \")\n",
    "                    if proceed == 'y' and not proceed.strip() == '':\n",
    "\n",
    "                        # ASCII FILE UPLOAD\n",
    "                        print (\"\\n\")\n",
    "                        response = upload_to_TNS(files)\n",
    "                        print (response)\n",
    "\n",
    "                        if not response:\n",
    "                            print(\"File upload didn't work\")\n",
    "                            print(response)\n",
    "                            #return False\n",
    "\n",
    "                        print(response['id_code'], response['id_message'],\n",
    "                              \"\\nSuccessfully uploaded ascii spectrum\")\n",
    "                        #classificationReport.asciiName = response['data'][-1]\n",
    "\n",
    "                        report_id = tns_classify(classificationReport)\n",
    "                        tns_feedback(report_id) \n",
    "                        post_comment(ztfname, 'Uploaded to TNS')\n",
    "\n",
    "\n",
    "                if inst == 'LRIS':\n",
    "\n",
    "                    classifiers = 'A. Dahiwale, C. Fremling(Caltech) on behalf of the Zwicky Transient Facility (ZTF)'### Change accordingly\n",
    "                    source_group = 48 ### Require source group id from drop down list, 0 is for None\n",
    "                    spectypes = np.array(['object','host','sky','arcs','synthetic'])\n",
    "\n",
    "                    header = (a['data']['altdata'])\n",
    "\n",
    "                    #proprietary_period = int(input(\"Proprietary period in years:\", x)\n",
    "                    proprietary_period = '0'\n",
    "                    proprietary_units = \"years\"\n",
    "                    spec_comments =''\n",
    "                    classification_comments = ''\n",
    "                    spectype='object'\n",
    "                    spectype_id = ['object', 'host', 'sky', 'arcs', 'synthetic'].index(spectype) + 1\n",
    "\n",
    "                    OBSDATE = str(a['data']['observed_at'].split('T')[0])+' '+str(a['data']['observed_at'].split('T')[1])\n",
    "\n",
    "                    classificationReport = TNSClassificationReport()\n",
    "                    classificationReport.name = get_IAUname(ztfname)[0]['name'][3:]  \n",
    "                    classificationReport.fitsName = ''\n",
    "                    classificationReport.asciiName = spectrum_name\n",
    "                    classificationReport.classifierName = classifiers\n",
    "                    classificationReport.classificationID = get_TNS_classification_ID(classify)\n",
    "                    classificationReport.redshift = get_redshift(ztfname)\n",
    "                    classificationReport.classificationComments = classification_comments\n",
    "                    classificationReport.obsDate = OBSDATE\n",
    "                    classificationReport.instrumentID = get_TNS_instrument_ID(inst)\n",
    "                    #classificationReport.expTime = '300'\n",
    "                    classificationReport.observers = (str(a['data']['observers'][0]['first_name'])+' '+str(a['data']['observers'][0]['last_name']))\n",
    "                    classificationReport.reducers = (str(a['data']['reducers'][0]['first_name'])+' '+str(a['data']['reducers'][0]['last_name']))\n",
    "                    classificationReport.specTypeID = spectype_id\n",
    "                    classificationReport.spectrumComments = spec_comments\n",
    "                    classificationReport.groupID = source_group\n",
    "                    classificationReport.spec_proprietary_period_value = proprietary_period\n",
    "                    classificationReport.spec_proprietary_period_units = proprietary_units\n",
    "                    print(classificationReport.classificationDict())\n",
    "\n",
    "                    #pprint(classificationReport.fill(), tab='  ')\n",
    "                    proceed = input(\"\\nProceed with classification and upload? ([y]/n) : \")\n",
    "                    if proceed == 'y' and not proceed.strip() == '':\n",
    "\n",
    "                        # ASCII FILE UPLOAD\n",
    "                        print (\"\\n\")\n",
    "                        response = upload_to_TNS(files)\n",
    "                        print (response)\n",
    "\n",
    "                        if not response:\n",
    "                            print(\"File upload didn't work\")\n",
    "                            print(response)\n",
    "                            #return False\n",
    "\n",
    "                        print(response['id_code'], response['id_message'],\n",
    "                              \"\\nSuccessfully uploaded ascii spectrum\")\n",
    "                        #classificationReport.asciiName = response['data'][-1]\n",
    "\n",
    "                        report_id = tns_classify(classificationReport)\n",
    "                        tns_feedback(report_id)\n",
    "                        post_comment(ztfname, 'Uploaded to TNS')\n",
    "\n",
    "\n",
    "                if inst == 'DIS':\n",
    "\n",
    "                    classifiers = 'Melissa L. Graham (UW), A. Dahiwale, C. Fremling(Caltech) on behalf of the Zwicky Transient Facility (ZTF)'### Change accordingly\n",
    "                    source_group = 48 ### Require source group id from drop down list, 0 is for None\n",
    "                    spectypes = np.array(['object','host','sky','arcs','synthetic'])\n",
    "                    #proprietary_period = int(input(\"Proprietary period in years:\", x)\n",
    "                    proprietary_period = '0'\n",
    "                    proprietary_units = \"years\"\n",
    "                    spec_comments =''\n",
    "                    classification_comments = ''\n",
    "                    spectype='object'\n",
    "                    spectype_id = ['object', 'host', 'sky', 'arcs', 'synthetic'].index(spectype) + 1\n",
    "\n",
    "                    #obsdate = APO(specid)[0]  \n",
    "                    #exptime = APO(specid)[1]\n",
    "                    #observers = APO(specid)[2]\n",
    "                    #reducers = APO(specid)[3]\n",
    "\n",
    "                    obsdate = str(a['data']['observed_at'].split('T')[0])+' '+str(a['data']['observed_at'].split('T')[1])\n",
    "\n",
    "                    classificationReport = TNSClassificationReport()\n",
    "                    classificationReport.name = get_IAUname(ztfname)[0]['name'][3:]  \n",
    "                    classificationReport.fitsName = ''\n",
    "                    classificationReport.asciiName = spectrum_name\n",
    "                    classificationReport.classifierName = classifiers\n",
    "                    classificationReport.classificationID = get_TNS_classification_ID(classify)\n",
    "                    classificationReport.redshift = get_redshift(ztfname)\n",
    "                    classificationReport.classificationComments = classification_comments\n",
    "                    classificationReport.obsDate = obsdate\n",
    "                    classificationReport.instrumentID = get_TNS_instrument_ID(inst)\n",
    "                    #classificationReport.expTime = exptime\n",
    "                    #classificationReport.observers = observers\n",
    "                    #classificationReport.reducers = reducers\n",
    "                    classificationReport.observers = (str(a['data']['observers'][0]['first_name'])+' '+str(a['data']['observers'][0]['last_name']))\n",
    "                    classificationReport.reducers = (str(a['data']['reducers'][0]['first_name'])+' '+str(a['data']['reducers'][0]['last_name']))\n",
    "                    classificationReport.specTypeID = spectype_id\n",
    "                    classificationReport.spectrumComments = spec_comments\n",
    "                    classificationReport.groupID = source_group\n",
    "                    classificationReport.spec_proprietary_period_value = proprietary_period\n",
    "                    classificationReport.spec_proprietary_period_units = proprietary_units\n",
    "                    print(classificationReport.classificationDict())\n",
    "\n",
    "                    #pprint(classificationReport.fill(), tab='  ')\n",
    "                    proceed = input(\"\\nProceed with classification and upload? ([y]/n) : \")\n",
    "                    if proceed == 'y' and not proceed.strip() == '':\n",
    "\n",
    "                        #ASCII FILE UPLOAD\n",
    "                        print (\"\\n\")\n",
    "                        response = upload_to_TNS(files)\n",
    "                        print (response)\n",
    "\n",
    "                        if not response:\n",
    "                            print(\"File upload didn't work\")\n",
    "                            print(response)\n",
    "                            #return False\n",
    "\n",
    "                        print(response['id_code'], response['id_message'],\n",
    "                              \"\\nSuccessfully uploaded ascii spectrum\")\n",
    "                        #classificationReport.asciiName = response['data'][-1]\n",
    "\n",
    "                        report_id = tns_classify(classificationReport)\n",
    "                        tns_feedback(report_id)\n",
    "                        post_comment(ztfname, 'Uploaded to TNS')\n",
    "                        \n",
    "                        \n",
    "                if inst == 'KAST':\n",
    "\n",
    "                    classifiers = 'Thomas G. Brink, Kishore C. Patra, Thomas de Jaeger, WeiKang Zheng, Benjamin E. Stahl, Alexei V. Filippenko (UC Berkeley), A. Dahiwale, C. Fremling (Caltech) on behalf of the Zwicky Transient Facility (ZTF)'### Change accordingly\n",
    "                    #classifiers = 'A. Dahiwale, C. Fremling(Caltech) on behalf of the Zwicky Transient Facility (ZTF)'### Change accordingly\n",
    "                    source_group = 48 ### Require source group id from drop down list, 0 is for None\n",
    "                    spectypes = np.array(['object','host','sky','arcs','synthetic'])\n",
    "                    #proprietary_period = int(input(\"Proprietary period in years:\", x)\n",
    "                    proprietary_period = '0'\n",
    "                    proprietary_units = \"years\"\n",
    "                    spec_comments =''\n",
    "                    classification_comments = ''\n",
    "                    spectype='object'\n",
    "                    spectype_id = ['object', 'host', 'sky', 'arcs', 'synthetic'].index(spectype) + 1\n",
    "\n",
    "\n",
    "                    obsdate = str(a['data']['observed_at'].split('T')[0])+' '+str(a['data']['observed_at'].split('T')[1])\n",
    "\n",
    "                    classificationReport = TNSClassificationReport()\n",
    "                    classificationReport.name = get_IAUname(ztfname)[0]['name'][3:]  \n",
    "                    classificationReport.fitsName = ''\n",
    "                    classificationReport.asciiName = spectrum_name\n",
    "                    classificationReport.classifierName = classifiers\n",
    "                    classificationReport.classificationID = get_TNS_classification_ID(classify)\n",
    "                    classificationReport.redshift = get_redshift(ztfname)\n",
    "                    classificationReport.classificationComments = classification_comments\n",
    "                    classificationReport.obsDate = obsdate\n",
    "                    classificationReport.instrumentID = get_TNS_instrument_ID(inst)\n",
    "                    #classificationReport.expTime = exptime\n",
    "                    classificationReport.observers = (str(a['data']['observers'][0]['first_name'])+' '+str(a['data']['observers'][0]['last_name']))\n",
    "                    classificationReport.reducers = (str(a['data']['reducers'][0]['first_name'])+' '+str(a['data']['reducers'][0]['last_name']))\n",
    "                    classificationReport.specTypeID = spectype_id\n",
    "                    classificationReport.spectrumComments = spec_comments\n",
    "                    classificationReport.groupID = source_group\n",
    "                    classificationReport.spec_proprietary_period_value = proprietary_period\n",
    "                    classificationReport.spec_proprietary_period_units = proprietary_units\n",
    "                    print(classificationReport.classificationDict())\n",
    "\n",
    "                    #pprint(classificationReport.fill(), tab='  ')\n",
    "                    proceed = input(\"\\nProceed with classification and upload? ([y]/n) : \")\n",
    "                    if proceed == 'y' and not proceed.strip() == '':\n",
    "\n",
    "                        #ASCII FILE UPLOAD\n",
    "                        print (\"\\n\")\n",
    "                        response = upload_to_TNS(files)\n",
    "                        print (response)\n",
    "\n",
    "                        if not response:\n",
    "                            print(\"File upload didn't work\")\n",
    "                            print(response)\n",
    "                            #return False\n",
    "\n",
    "                        print(response['id_code'], response['id_message'],\n",
    "                              \"\\nSuccessfully uploaded ascii spectrum\")\n",
    "                        #classificationReport.asciiName = response['data'][-1]\n",
    "\n",
    "                        report_id = tns_classify(classificationReport)\n",
    "                        tns_feedback(report_id)\n",
    "                        post_comment(ztfname, 'Uploaded to TNS')\n",
    "                        \n",
    "                        #print('deleting files now')\n",
    "                        #call(['rm', all_info['asciifile']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
